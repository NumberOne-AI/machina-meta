# Evidence Report: Gemini 3 Max Tokens Error

**Date**: 2026-01-24
**Related Problem**: [PROBLEMS.md - Gemini 3 Max Tokens Error](../../PROBLEMS.md) (Workspace - Infrastructure)
**Environment**: tusdi-preview-92

## Summary

The `gemini-3-pro-preview` model is failing with `FinishReason.MAX_TOKENS` during document processing. This indicates the response generation was truncated because it hit the token limit (default or configured), resulting in an empty or incomplete response that the application treats as a failure.

## Raw Evidence

**Source**: `tusdi-preview-92` logs (tusdi-api)
**Timestamp**: 2026-01-24 02:15:23 UTC

```text
2026-01-24 02:15:23 [warning  ] gemini_empty_response          finish_reason=FinishReason.MAX_TOKENS model=gemini-3-pro-preview output_tokens=0 safety_ratings=None
2026-01-24 02:15:23 [error    ] llm_request_failed             error='Gemini returned empty response (finish_reason=FinishReason.MAX_TOKENS). This may indicate content policy violation or model refusal.' model=vertex_ai/gemini-3-pro-preview request_id=GH2whe2I session_id=3mCAEcWIBS36
```

## Context

- **Model**: `vertex_ai/gemini-3-pro-preview`
- **Document ID**: `eec9c742-4025-4620-830d-b52eebe37264`
- **Filename**: `page_007.png` (Single page image)
- **Output Tokens**: 0 (in warning log), likely meaning the *useful* output was 0 because it hit the limit immediately or during "thinking" phase? Or potentially the logging of `output_tokens=0` is a side effect of the exception.
- **Session ID**: `3mCAEcWIBS36`
- **Request ID**: `GH2whe2I`

## Analysis

The error explicitly cites `finish_reason=FinishReason.MAX_TOKENS`. 
Gemini 3.0 has a "thinking" capability that consumes output tokens. If the `max_tokens` parameter is set too low (e.g., default 8,192 or similar legacy defaults), the extensive reasoning generated by the model might exhaust the budget before producing the final JSON output.

The fact that this occurred on a single page image (`page_007.png`) suggests that the "thinking" process was verbose enough to hit the limit, or the page contained dense data requiring a large output schema. The previous limit was likely 8,192 tokens. Gemini 3 Pro supports up to 65,536 output tokens.

## Remediation

We have implemented controls to increase the `max_tokens` limit to 65,536 (Gemini 3 maximum) and exposed `thinking_level` configurations to manage the reasoning budget.

See: `TODO.md` -> "Enhance docproc generic parser with Gemini 3 controls"
